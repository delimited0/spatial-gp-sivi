
@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2018-06-01},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning},
	pages = {859--877},
	file = {arXiv.org Snapshot:/Users/patrickding/Zotero/storage/FNEGPABN/1601.html:text/html;Blei et al_2017_Variational Inference.pdf:/Users/patrickding/Zotero/storage/6IK64W72/Blei et al_2017_Variational Inference.pdf:application/pdf}
}
@inproceedings{rezende_variational_2015,
	address = {Lille, France},
	series = {{ICML}'15},
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045281},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2018-06-01},
	booktitle = {Proceedings of the 32Nd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37},
	publisher = {JMLR.org},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	year = {2015},
	pages = {1530--1538},
	file = {Rezende and Mohamed - Variational Inference with Normalizing Flows.pdf:/Users/patrickding/Zotero/storage/7YPKPVC6/Rezende and Mohamed - Variational Inference with Normalizing Flows.pdf:application/pdf}
}

@inproceedings{ranganath_hierarchical_2016,
	title = {Hierarchical {Variational} {Models}},
	url = {http://proceedings.mlr.press/v48/ranganath16.html},
	abstract = {Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central questio...},
	language = {en},
	urldate = {2018-06-01},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Ranganath, Rajesh and Tran, Dustin and Blei, David},
	month = jun,
	year = {2016},
	pages = {324--333},
	file = {Full Text PDF:/Users/patrickding/Zotero/storage/J8XVAMVC/Ranganath et al. - 2016 - Hierarchical Variational Models.pdf:application/pdf;Snapshot:/Users/patrickding/Zotero/storage/9JXJUDTY/ranganath16.html:text/html}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2018-06-01},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, \_tablet\_modified},
	file = {arXiv.org Snapshot:/Users/patrickding/Zotero/storage/3PHYF2KZ/1312.html:text/html;Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf:/Users/patrickding/Zotero/storage/N76QQS3V/Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf:application/pdf}
}

@inproceedings{salimans_markov_2015,
	title = {Markov {Chain} {Monte} {Carlo} and {Variational} {Inference}: {Bridging} the {Gap}},
	shorttitle = {Markov {Chain} {Monte} {Carlo} and {Variational} {Inference}},
	url = {http://proceedings.mlr.press/v37/salimans15.html},
	abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. Thi...},
	language = {en},
	urldate = {2018-06-01},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
	month = jun,
	year = {2015},
	keywords = {\_tablet},
	pages = {1218--1226},
	file = {Salimans et al_2015_Markov Chain Monte Carlo and Variational Inference.pdf:/Users/patrickding/Zotero/storage/XJZN75T7/Salimans et al_2015_Markov Chain Monte Carlo and Variational Inference.pdf:application/pdf;Snapshot:/Users/patrickding/Zotero/storage/PHYUMDN7/salimans15.html:text/html}
}

@article{mescheder_adversarial_2017,
	title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	shorttitle = {Adversarial {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1701.04722},
	abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	urldate = {2018-06-20},
	journal = {arXiv:1701.04722 [cs]},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04722},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/patrickding/Zotero/storage/9CPISHLG/1701.html:text/html;Mescheder et al_2017_Adversarial Variational Bayes.pdf:/Users/patrickding/Zotero/storage/CWTEN5GW/Mescheder et al_2017_Adversarial Variational Bayes.pdf:application/pdf}
}

@book{jaakkola_improving_1998,
	title = {Improving the {Mean} {Field} {Approximation} via the {Use} of {Mixture} {Distributions}},
	abstract = {. Mean field methods provide computationally efficient approximations to posterior probability distributions for graphical models. Simple mean field methods make a completely factorized approximation to the posterior, which is unlikely to be accurate when the posterior is multimodal. Indeed, if the posterior is multi-modal, only one of the modes can be captured. To improve the mean field approximation in such cases, we employ mixture models as posterior approximations, where each mixture component is a factorized distribution. We describe efficient methods for optimizing the parameters in these models. 1. Introduction  Graphical models provide a convenient formalism in which to express and manipulate conditional independence statements. Inference algorithms for graphical models exploit these independence statements, using them to compute conditional probabilities while avoiding brute force marginalization over the joint probability table. Many inference algorithms, in particular the al...},
	author = {Jaakkola, Tommi S. and Jordan, Michael I.},
	year = {1998},
	file = {Citeseer - Snapshot:/Users/patrickding/Zotero/storage/AEM5TI4W/summary.html:text/html;Jaakkola_Jordan_1998_Improving the Mean Field Approximation via the Use of Mixture Distributions.pdf:/Users/patrickding/Zotero/storage/IVP24EJZ/Jaakkola_Jordan_1998_Improving the Mean Field Approximation via the Use of Mixture Distributions.pdf:application/pdf}
}

@article{tran_variational_2015,
	title = {The {Variational} {Gaussian} {Process}},
	url = {http://arxiv.org/abs/1511.06499},
	abstract = {Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.},
	urldate = {2018-06-20},
	journal = {arXiv:1511.06499 [cs, stat]},
	author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06499},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Appears in International Conference on Learning Representations, 2016},
	file = {arXiv.org Snapshot:/Users/patrickding/Zotero/storage/FRBCG3RE/1511.html:text/html;Tran et al_2015_The Variational Gaussian Process.pdf:/Users/patrickding/Zotero/storage/REYT27IB/Tran et al_2015_The Variational Gaussian Process.pdf:application/pdf}
}

@incollection{tran_hierarchical_2017,
	title = {Hierarchical {Implicit} {Models} and {Likelihood}-{Free} {Variational} {Inference}},
	url = {http://papers.nips.cc/paper/7136-hierarchical-implicit-models-and-likelihood-free-variational-inference.pdf},
	urldate = {2018-06-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Tran, Dustin and Ranganath, Rajesh and Blei, David},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {5523--5533},
	file = {NIPS Snapshort:/Users/patrickding/Zotero/storage/VV7LPXX2/7136-hierarchical-implicit-models-and-likelihood-free-variational-inference.html:text/html;Tran et al_2017_Hierarchical Implicit Models and Likelihood-Free Variational Inference.pdf:/Users/patrickding/Zotero/storage/BVZC83J3/Tran et al_2017_Hierarchical Implicit Models and Likelihood-Free Variational Inference.pdf:application/pdf}
}

@incollection{tran_copula_2015,
	title = {Copula variational inference},
	url = {http://papers.nips.cc/paper/5669-copula-variational-inference.pdf},
	urldate = {2018-06-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Tran, Dustin and Blei, David and Airoldi, Edo M},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {3564--3572},
	file = {NIPS Snapshort:/Users/patrickding/Zotero/storage/LHUA5L56/5669-copula-variational-inference.html:text/html;Tran et al_2015_Copula variational inference.pdf:/Users/patrickding/Zotero/storage/35II4EFP/Tran et al_2015_Copula variational inference.pdf:application/pdf}
}

@inproceedings{miller_variational_2017,
	title = {Variational {Boosting}: {Iteratively} {Refining} {Posterior} {Approximations}},
	shorttitle = {Variational {Boosting}},
	url = {http://proceedings.mlr.press/v70/miller17a.html},
	abstract = {We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, variational boosting, iteratively refines an ...},
	language = {en},
	urldate = {2018-06-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Miller, Andrew C. and Foti, Nicholas J. and Adams, Ryan P.},
	month = jul,
	year = {2017},
	pages = {2420--2429},
	file = {Miller et al_2017_Variational Boosting.pdf:/Users/patrickding/Zotero/storage/M5PM7IXV/Miller et al_2017_Variational Boosting.pdf:application/pdf;Snapshot:/Users/patrickding/Zotero/storage/3N9FZRAH/miller17a.html:text/html}
}

@inproceedings{yin_semi-implicit_2018,
	address = {Stockholm, Sweden},
	title = {Semi-{Implicit} {Variational} {Inference}},
	abstract = {Semi-implicit variational inference (SIVI) is introduced to expand the commonly used analytic variational distribution family, by mixing the variational parameter with a ﬂexible distribution. This mixing distribution can assume any density function, explicit or not, as long as independent random samples can be generated via reparameterization. Not only does SIVI expand the variational family to incorporate highly ﬂexible variational distributions, including implicit ones that have no analytic density functions, but also sandwiches the evidence lower bound (ELBO) between a lower bound and an upper bound, and further derives an asymptotically exact surrogate ELBO that is amenable to optimization via stochastic gradient ascent. With a substantially expanded variational family and a novel optimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the posterior in a variety of Bayesian inference tasks.},
	language = {en},
	booktitle = {Proceedings of the 35 th {International} {Conference} on {Machine} {Learning}},
	author = {Yin, Mingzhang and Zhou, Mingyuan},
	month = jul,
	year = {2018},
	pages = {16},
	file = {Yin and Zhou - Semi-Implicit Variational Inference.pdf:/Users/patrickding/Zotero/storage/4E5SDQ3K/Yin and Zhou - Semi-Implicit Variational Inference.pdf:application/pdf}
}

@article{titsias_unbiased_2018,
	title = {Unbiased {Implicit} {Variational} {Inference}},
	url = {https://arxiv.org/abs/1808.02078v3},
	abstract = {We develop unbiased implicit variational inference (UIVI), a method that
expands the applicability of variational inference by defining an expressive
variational family. UIVI considers an implicit variational distribution
obtained in a hierarchical manner using a simple reparameterizable distribution
whose variational parameters are defined by arbitrarily flexible deep neural
networks. Unlike previous works, UIVI directly optimizes the evidence lower
bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on
several models, including Bayesian multinomial logistic regression and
variational autoencoders, and show that UIVI achieves both tighter ELBO and
better predictive performance than existing approaches at a similar
computational cost.},
	language = {en},
	urldate = {2019-03-19},
	author = {Titsias, Michalis K. and Ruiz, Francisco J. R.},
	month = aug,
	year = {2018},
	file = {Snapshot:/Users/patrickding/Zotero/storage/SHA5R9EN/1808.html:text/html;Titsias_Ruiz_2018_Unbiased Implicit Variational Inference.pdf:/Users/patrickding/Zotero/storage/99H2UCVC/Titsias_Ruiz_2018_Unbiased Implicit Variational Inference.pdf:application/pdf}
}

@article{shi_kernel_2017,
	title = {Kernel {Implicit} {Variational} {Inference}},
	url = {https://arxiv.org/abs/1705.10119v3},
	abstract = {Recent progress in variational inference has paid much attention to the
flexibility of variational posteriors. One promising direction is to use
implicit distributions, i.e., distributions without tractable densities as the
variational posterior. However, existing methods on implicit posteriors still
face challenges of noisy estimation and computational infeasibility when
applied to models with high-dimensional latent variables. In this paper, we
present a new approach named Kernel Implicit Variational Inference that
addresses these challenges. As far as we know, for the first time implicit
variational inference is successfully applied to Bayesian neural networks,
which shows promising results on both regression and classification tasks.},
	language = {en},
	urldate = {2019-03-20},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = may,
	year = {2017},
	file = {Shi et al_2017_Kernel Implicit Variational Inference.pdf:/Users/patrickding/Zotero/storage/LRR4X4C2/Shi et al_2017_Kernel Implicit Variational Inference.pdf:application/pdf;Snapshot:/Users/patrickding/Zotero/storage/47PPSL3R/1705.html:text/html}
}

@article{huszar_variational_2017,
	title = {Variational {Inference} using {Implicit} {Distributions}},
	url = {http://arxiv.org/abs/1702.08235},
	abstract = {Generative adversarial networks (GANs) have given us a great tool to fit implicit generative models to data. Implicit distributions are ones we can sample from easily, and take derivatives of samples with respect to model parameters. These models are highly expressive and we argue they can prove just as useful for variational inference (VI) as they are for generative modelling. Several papers have proposed GAN-like algorithms for inference, however, connections to the theory of VI are not always well understood. This paper provides a unifying review of existing algorithms establishing connections between variational autoencoders, adversarially learned inference, operator VI, GAN-based image reconstruction, and more. Secondly, the paper provides a framework for building new algorithms: depending on the way the variational bound is expressed we introduce prior-contrastive and joint-contrastive methods, and show practical inference algorithms based on either density ratio estimation or denoising.},
	urldate = {2019-03-27},
	journal = {arXiv:1702.08235 [cs, stat]},
	author = {Huszár, Ferenc},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08235},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/patrickding/Zotero/storage/ELJ6NP4T/1702.html:text/html;Huszár_2017_Variational Inference using Implicit Distributions.pdf:/Users/patrickding/Zotero/storage/B29A8YA8/Huszár_2017_Variational Inference using Implicit Distributions.pdf:application/pdf}
}